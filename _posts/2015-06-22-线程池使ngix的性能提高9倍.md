---
layout: post
title: '线程池使NGINX的性能提高9倍--异步server的进化'
---

##   介绍
	
众所周知，NGINX采用异步、事件驱动的方法来处理请求。与传统的服务器架构为每个请求创建另一个服务的进程或线程不同，NGINX使用非阻塞的套接字和epoll/kqueue模式在一个工作进程中处理多个请求。而且服务器上进程的数量是一个非常小的常量，使用的内存不多，也不会消耗CPU在进程切换上，因此类似NGINX的服务器架构可以同时处理上百万的模拟请求。

![]({{site.baseurl}}/img/622-1.png)

即使如此，异步、事件驱动的方法仍然不完美。导致这个缺陷的原因是：阻塞。不幸的是，很多第三方模块使用阻塞调用，而用户并了解这些特性，导致NGINX的性能严重下降。因此必须不惜一切代价避免阻塞。
但是目前即使在NGINX官方的代码里都不能做到完全非阻塞。解决这个问题的机制已经在NGINX版本1.7.11中实现。这个机制就是“线程池”。下面我们先来聊聊阻塞。

##   阻塞

为了更好地理解这一问题，先简要介绍下NGINX的工作机制。
总的来说，NGINX是事件处理程序，从内核中接收请求产生的事件信息，然后告诉操作系统如何处理的控制器。NGINX把所有繁重任务都交给操作系统，使NGINX可以快速、及时响应。

![]({{site.baseurl}}/img/622-2.png)

事件可以是超时、通知套接字读写数据、通知错误告警等。NGINX收到一堆的事件并放入处理队列中，然后一个接一个的处理它们。事件会在线程的主循环中完成。在大多数情况下，事件处理非常快，也许只需要几个CPU周期。

![]({{site.baseurl}}/img/622-3.png)

如果有些事件处理非常耗时，将会发生什么呢？整个事件处理循环会等待这个事件处理完成后才会继续下去。
所以，阻塞操作意味着事件处理循环将会被这个操作停止一段较长的时间。阻塞操作有很多种，例如，CPU密集型计算、等待锁、磁盘IO、同步的网络操作等。关键是当进行阻塞操作时，工作进程不能处理其他事件，即使系统资源空闲。
举个例子，有一个队顾客正在排队购买销售员的商品。第一个顾客需要购买的物品在这间商店没有库存，于是销售员立马跑到仓库去取货。整个队伍的人将要等待两个小时才能购买，即使他们需要的商品就在商店中。你能想象后面的等待的人的表情吗？那会是非常烦躁不安的。

![]({{site.baseurl}}/img/622-4.png)

同样的情况会发生的NGINX中。当读取的数据不在内存中，需要从磁盘的文件中获取。磁盘IO耗时几百个CPU周期，而队列中的其他请求需要的数据可能就在内存中，但是他们必须等待。结果大部分的请求的延迟增加，系统资源利用率下降。

![]({{site.baseurl}}/img/622-5.png)

有些操作系统，例如FreeBSD,提供读写文件的异步接口，NGINX就能利用这些接口。但是不是所有的Linux系统都提供类似的接口。虽然Linux提供了许多读取文件的异步接口，但是这里有两个明显的坑。一个是文件需要和buffer对齐，另一个是文件描述符需要在`O_DIRECT`置位。第一个坑NGINX能很好处理，而第二个坑意味着所有的文件访问都会增加磁盘负载，这是不科学的。
为了解决这个问题，线程池技术引入了NGINX 1.7.11。但是现在默认的NGINX Plus并没有包含这个特性，如果你需要需要联系NGINX的销售去构建NGINX Plus R6包含线程池的版本。
下面介绍下线程池技术及其原理。

##   线程池

我们回到笨销售员的例子。这回销售员变聪明了，他叫了个快递去取仓库中的商品，并让这个顾客在一旁等待，继续处理后面顾客的需求。因此只要需要购买仓库中的商品的顾客需要等待，其他顾客的需求可以得到马上满足。

![]({{site.baseurl}}/img/622-6.png)

在NGINX中，线程池的功能就和快递一样。它由一个任务队列和几个处理线程组成。当工作进程需要处理事件有耗时的任务时，就把任务放进线程池的任务队列，然后这个事件就会被空闲的线程处理。

![]({{site.baseurl}}/img/622-7.jpg)
 
这个任务队列被一些特定的资源限制，可能会处理不过来。但是至少耗时的事件不会阻碍其他事件的处理。
磁盘IO是阻塞操作最常见的例子。NGINX中的线程池可以处理任何不适合在工作进程主循环中处理的任务。
现在，线程池中只实现了两个常见的操作，大多少系统中的`read()`系统调用和Linux系统中的`sendfile()`系统调用。NIGIX将会继续在未来的版本中增加相关实现操作在线程池中的实现。

##   基准测试

实践是检验真理的唯一标准。我们将模拟阻塞和非阻塞的混合请求来对线程池的效率做基准测试。
请求需要获取数据，但是数据不全在内存中。测试机有48GB内存，我们在磁盘上生成了一共256GB的4MB文件，然后配置NGINX 1.9.0。

<pre><code>
worker_processes 16;

events {
    accept_mutex off;
}

http {
    include mime.types;
    default_type application/octet-stream;

    access_log off;
    sendfile on;
    sendfile_max_chunk 512k;

    server {
        listen 8000;

        location / {
            root /storage;
        }
    }
}	
</code></pre>

为了达到更好的性能`logging`和`accept_mutex`已经被禁用,`sendfile`和`sendfile_max_chunk`被设置。`sendfile_max_chunk`可以减少sendfile()的耗时，因为NGINX每次只发送512KB的块。
测试机有两个Intel Xeon E5645（12 cores，24 HT-threads in total）处理器，10Gbps网卡。有四块Western Digital WD1003FBYX硬盘，并组成RAID10阵列。操作系统是Ubuntu Server 14.04.1 LTS。

![]({{site.baseurl}}/img/622-9.jpg)

客户端有两台一样的测试机。其中一台并发200，以随机的顺序请求文件。这些请求可能会导致缓存未命中而产生磁盘IO。这台机器我们称为随机负载。另一台并发50，请求同一个文件。由于文件频繁访问，会被缓存在内存中。NGINX会迅速的处理这类请求，但是可能会被其他请求阻塞。这台机器我们称为常量负载。
在服务器上使用ifstat监测性能。并在第二天测试机上使用WRK监测性能。

没有线程池的版本性能一般：

![]({{site.baseurl}}/img/622-10.png)

无线程池的版本的吞吐量为1Gbps。top输出的信息显示工作线程都在阻塞状态。

![]({{site.baseurl}}/img/622-11.jpg)

吞吐量被磁盘IO限制，CPU大部分时间都处于空闲状态。
WRK的结果非常糟糕。

![]({{site.baseurl}}/img/622-12.png)

常量负载请求的数据应该是从内存中获取，但是延迟非常大。这是由于所有的工作进程都在处理随机负载产生的磁盘IO。

现在使用线程池版本测试。在配置的`location`块中加入`aio threads`。

<pre><code>
location / {
    root /storage;
    aio threads;
}	
</code></pre>

测试结果

![]({{site.baseurl}}/img/622-14.jpg)

线程池的版本吞吐量为9.5Gbps！
可能可以达到更高的吞吐量，但是9.5Gbps已经接近网卡的极限吞吐量。这个测试NGINX的瓶颈在网卡。top输出的信息显示工作线程都在休眠等待新的事件。

![]({{site.baseurl}}/img/622-15.jpg)

CPU大部分时间任然处于空闲状态。
WRK的结果。

![]({{site.baseurl}}/img/622-16.jpg)

平均延迟已经由7.42秒下降到226.32毫秒，速度提升33倍。每秒的请求由8上升到250，数量提升了31倍！
这结果是由于请求不再受到其他阻塞操作的影响。只要磁盘系统工作正常，随机负载就能得到很好的服务。NGINX使用剩下的资源来服务常量负载。

##   依旧没有银弹

服务中有阻塞操作和有提升性能的需求，你可能会迫不及待的使用线程池。等一下，先别急。

服务中大部分`read()`和`sendfile()`操作不会产生太多的磁盘IO。如果你有足够的内存，操作系统会自动把访问频率高的文件缓存在内存中，我们称为页缓存。

页缓存非常有效，使NGINX在大部分情景下有良好性能。页缓存中取数据十分快，这样使用现场池会有点多余。

所以如果你有足够的内存和所处理的数据总量不大，在没有线程池的情况下已经可以很完美的工作了。

只有在特定的场景下，才需要把`read()`交给线程池处理。当出现大量请求获取数据，但数据不能保存在系统的VM cache中时，使用线程池会有明显的效果。最常见的例子就流媒体服务，这个和我们基准测试情况差不多。

我们非常欣喜，能把`read()`操作交给线程池处理从而降低负载。判断是否使用线程池的标准是文件数据是否能保存在内存中。目前遇到的情况总，只有上面的例子需要使用线程池来处理`read()`事件。

再说说刚刚的销售员例子。现在销售员不知道商品是否在商店中，因此他要么把每个商品都让快递员去取，要么都自己去取。

问题的根源是操作系统忽略了这个特性。在2010年Linux尝试加入`fincore()`系统调用，但是最终未能实现。之后数度尝试了在`preadv2()`中加入`RWF_NONBLOCK`标记。但是这些情况仍不明朗。内核还没加入这个特性的主要原因是拖延。

FreeBSD已经有非常好的异步read接口，因此不需要使用线程池。

##   配置线程池

如果你确定使用线程池能提高性能，下面来介绍下线程池的配置。

配置十分简单灵活。首先你需要使用NGINX 1.7.11版本或者之后的版本，带上`--with-threads`标志编译。在最简单的例子，只需要在`http`, `server`, 或`location`块中加入`aio threads`。

<pre><code>
aio threads;
</code></pre>

这是最简单的线程池配置，其实是以下配置的简化版。

<pre><code>
thread_pool default threads=32 max_queue=65536;
aio threads=default;
</code></pre>

配置定义了有65536个任务的任务队列和32个工作线程。如果任务队列满载，NGINX会打印错误日志并拒接新的请求。

<pre><code>
thread pool "NAME" queue overflow: N tasks waiting
</code></pre>

出现错误日志，意味着处理请求的速度已经远远低于请求增加的速度，你可以考虑增加工作线程，如果这样也不能解决问题，说明你的系统已经不能承载这么大量的请求。

使用`thread_pool`标记，你可以配置工作线程数量，任务队列长度，线程池的名字。线程池的名字可以把对应的线程池分配给指定的服务，以适应特定的需求。

<pre><code>
http {
    thread_pool one threads=128 max_queue=0;
    thread_pool two threads=32;

    server {
        location /one {
            aio threads=one;
        }

        location /two {
            aio threads=two;
        }
    }
…
}
</code></pre>


如果没有配置`max_queue`的值，NGXIN将使用默认的65536。`max_queue`可以设置为0，这时没有任务会在队列中等待，线程池只能处理线程数量的任务，其余的会被拒绝。

现在我们设定一个场景，一台有三个磁盘的缓存代理，缓存所有从后端来的回包，但是需要缓存的数据量将会超过内存的容量。这是一台CDN的节点，目标是最大化磁盘的性能。

你的一个选择是配置RAID阵列，或者配置一个特殊的NGINX。

<pre><code>
# We assume that each of the hard drives is mounted on one of the directories:
# /mnt/disk1, /mnt/disk2, or /mnt/disk3 accordingly
proxy_cache_path /mnt/disk1 levels=1:2 keys_zone=cache_1:256m max_size=1024G 
                 use_temp_path=off;
proxy_cache_path /mnt/disk2 levels=1:2 keys_zone=cache_2:256m max_size=1024G 
                 use_temp_path=off;
proxy_cache_path /mnt/disk3 levels=1:2 keys_zone=cache_3:256m max_size=1024G 
                 use_temp_path=off;

thread_pool pool_1 threads=16;
thread_pool pool_2 threads=16;
thread_pool pool_3 threads=16;

split_clients $request_uri $disk {
    33.3%     1;
    33.3%     2;
    *         3;
}

location / {
    proxy_pass http://backend;
    proxy_cache_key $request_uri;
    proxy_cache cache_$disk;
    aio threads=pool_$disk;
    sendfile on;
}
</code></pre>

这个配置使用了三个独立的缓存和三个独立的线程池分别服务三个磁盘。

`split_clients`配置起到了负载均衡的作用。

`proxy_cache_path`的`use_temp_path=off`参数指示NGINX将临时文件保存在对应的目录，避免相同的请求使文件在不同的磁盘中拷贝。

这样就能使磁盘达到最大的性能，因为NGINX通过三个线程池独立并行的服务对应的磁盘。每个磁盘有独立的16个线程和对应的任务队列来读写数据。

这个例子说明NGINX可以根据你的硬件需求进行配置。一个优秀的NGINX配置能使你的软件、操作系统、硬件一起工作达到最大的资源利用率。

##  总结

总的来说，线程池是一个很好的特性，它会使NGINX处理阻塞操作尤其是读取大量零碎的数据时性能达到一个新的高度。

不仅如此，前面提到其他的阻塞操作使用这个特性会同样提高性能。NGINX将会继续完善相关组件。许多流行的没有提供异步非阻塞的接口的函数库将会不兼容NGINX。NGINX将会投入更多的时间和资源开发自己的非阻塞原型库。随着线程池的上线，这些库在不影响模块性能的前提下会更加简单易用。               


[阅读原文](http://nginx.com/blog/thread-pools-boost-performance-9x/)
